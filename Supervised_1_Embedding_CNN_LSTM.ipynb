{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Supervised_1_Embedding_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWUeIH0oC_uW",
        "outputId": "fab4d132-1b9c-4654-ef78-fb48049b083f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl7xrhRvGJ54"
      },
      "source": [
        "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ6c8LktGxn_"
      },
      "source": [
        "# !unzip /content/crawl-300d-2M.vec.zip"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz6wXyOnCyvD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
        "\n",
        "import io\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgnc1wehC-SM"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Clean_Dataset.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-ve0toOcDNNM",
        "outputId": "327b33d8-fdb3-4d42-9bff-32e848b02e97"
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_created</th>\n",
              "      <th>date_created</th>\n",
              "      <th>up_votes</th>\n",
              "      <th>down_votes</th>\n",
              "      <th>title</th>\n",
              "      <th>over_18</th>\n",
              "      <th>author</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1201232046</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>score kill pakistan clash</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1201232075</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>japan resume refuel mission</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1201232523</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>us press egypt gaza border</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1201233290</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>jumpstart economy give health care</td>\n",
              "      <td>False</td>\n",
              "      <td>fadi420</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1201274720</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>council europe bash euun terror blacklist</td>\n",
              "      <td>False</td>\n",
              "      <td>mhermans</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509231</th>\n",
              "      <td>1479816764</td>\n",
              "      <td>2016-11-22</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>heil trump donald trump altright white nationa...</td>\n",
              "      <td>False</td>\n",
              "      <td>nonamenoglory</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509232</th>\n",
              "      <td>1479816772</td>\n",
              "      <td>2016-11-22</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>people speculate could madeleine mccann</td>\n",
              "      <td>False</td>\n",
              "      <td>SummerRay</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509233</th>\n",
              "      <td>1479817056</td>\n",
              "      <td>2016-11-22</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>professor receive arab researchers award</td>\n",
              "      <td>False</td>\n",
              "      <td>AUSharjah</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509234</th>\n",
              "      <td>1479817157</td>\n",
              "      <td>2016-11-22</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>nigel farage attack response trump ambassador ...</td>\n",
              "      <td>False</td>\n",
              "      <td>smilyflower</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509235</th>\n",
              "      <td>1479817346</td>\n",
              "      <td>2016-11-22</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>palestinian wield knife shoot dead west bank i...</td>\n",
              "      <td>False</td>\n",
              "      <td>superislam</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>509236 rows Ã— 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        time_created date_created  up_votes  ...  over_18         author   category\n",
              "0         1201232046   2008-01-25         3  ...    False          polar  worldnews\n",
              "1         1201232075   2008-01-25         2  ...    False          polar  worldnews\n",
              "2         1201232523   2008-01-25         3  ...    False          polar  worldnews\n",
              "3         1201233290   2008-01-25         1  ...    False        fadi420  worldnews\n",
              "4         1201274720   2008-01-25         4  ...    False       mhermans  worldnews\n",
              "...              ...          ...       ...  ...      ...            ...        ...\n",
              "509231    1479816764   2016-11-22         5  ...    False  nonamenoglory  worldnews\n",
              "509232    1479816772   2016-11-22         1  ...    False      SummerRay  worldnews\n",
              "509233    1479817056   2016-11-22         1  ...    False      AUSharjah  worldnews\n",
              "509234    1479817157   2016-11-22         1  ...    False    smilyflower  worldnews\n",
              "509235    1479817346   2016-11-22         1  ...    False     superislam  worldnews\n",
              "\n",
              "[509236 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xINmquvTDsbL"
      },
      "source": [
        "# Supervised - Predicting number of up votes a post might get"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxncaFZMDiyz"
      },
      "source": [
        "MAX_LEN = 12\n",
        "TRAIN_BATCH_SIZE = 2\n",
        "VALID_BATCH_SIZE = 2\n",
        "EPOCHS = 10"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUaE-7TADkUC"
      },
      "source": [
        "class Dataset:\n",
        "  def __init__(self, title, up_votes):\n",
        "    \"\"\"\n",
        "      :param title: this is a numpy array\n",
        "      :param up_votes: a vector, numpy array\n",
        "    \"\"\"\n",
        "    self.title = title\n",
        "    self.up_votes = up_votes\n",
        "    \n",
        "  def __len__(self):\n",
        "    # returns length of the dataset\n",
        "    return len(self.title)\n",
        "    \n",
        "  def __getitem__(self, item):\n",
        "    # for any given item, which is an int,\n",
        "    # return review and targets as torch tensor\n",
        "    # item is the index of the item in concern\n",
        "    title = self.title[item, :]\n",
        "    up_votes = self.up_votes[item]\n",
        "    return {\n",
        "      \"title\": torch.tensor(title, dtype=torch.long),\n",
        "      \"up_votes\": torch.tensor(up_votes, dtype=torch.float)\n",
        "    }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2opKyx1yD7KR"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Emb_CNN_LSTM_Model(torch.nn.Module):\n",
        "    def __init__(self, input_dim=0, embedding_dim=12, hidden_dim=20, output_dim=1,\n",
        "                 batch_size=8, num_layers=2, bidirectional=False, dropout=0):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.dropout = dropout\n",
        "        self.fc_hidden_dim = self.hidden_dim\n",
        "\n",
        "        if self.bidirectional:\n",
        "            self.fc_hidden_dim = self.hidden_dim * 2\n",
        "\n",
        "        self.embedding = nn.Embedding(self.input_dim, self.embedding_dim)\n",
        "\n",
        "        self.conv1D1 = nn.Conv1d(embedding_dim, 32, 5)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.maxp1 = nn.MaxPool1d(5)\n",
        "\n",
        "        self.dropout2 = nn.Dropout(0.4)\n",
        "        self.conv1D2 = nn.Conv1d(32, 64, 5)\n",
        "        self.maxp2 = nn.MaxPool1d(5)\n",
        "        self.dropout3 = nn.Dropout(0.4)\n",
        "        \n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(64, 32)\n",
        "        self.dropout4 = nn.Dropout(0.3)\n",
        "        # linear\n",
        "        self.dense = nn.Linear(128, 1)\n",
        "\n",
        "\n",
        "    def forward(self, opcode):\n",
        "        embedded = self.embedding(opcode)\n",
        "        # CNN\n",
        "        x = nn.Dropout(0.3)(embedded)\n",
        "        # cnn_x = x.unsqueeze(1)\n",
        "        cnn_x = self.conv1D1(x)\n",
        "        cnn_x = torch.tanh(cnn_x) \n",
        "        cnn_x = self.dropout1(cnn_x)\n",
        "        cnn_x = self.maxp1(cnn_x)\n",
        "        cnn_x = self.dropout2(cnn_x)\n",
        "        cnn_x = self.conv1D2(cnn_x)\n",
        "        cnn_x = torch.tanh(cnn_x) \n",
        "        cnn_x = self.maxp2(cnn_x)\n",
        "        \n",
        "        cnn_x = torch.transpose(cnn_x, 1, 2)\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(cnn_x)\n",
        "        lstm_out = self.dropout4(torch.transpose(lstm_out, 1, 2).squeeze(2))\n",
        "        # linear\n",
        "        lstm_out = lstm_out.reshape((lstm_out.shape[0], -1, 1)).squeeze()\n",
        "        cnn_lstm_out = self.dense(lstm_out)\n",
        "        \n",
        "        return cnn_lstm_out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mduOzvPEBpv"
      },
      "source": [
        "def get_scheduler(optimizer, scheduler):\n",
        "  if scheduler=='ReduceLROnPlateau':\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=4, verbose=True, eps=1e-6)\n",
        "  elif scheduler=='CosineAnnealingLR':\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
        "  elif scheduler=='CosineAnnealingWarmRestarts':\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1)\n",
        "  return scheduler\n",
        "\n",
        "def train(data_loader, model, optimizer, device, scheduler):\n",
        "  \"\"\"\n",
        "    This is the main training function that trains model\n",
        "    for one epoch\n",
        "    :param data_loader: this is the torch dataloader\n",
        "    :param model: model (lstm model)\n",
        "    :param optimizer: torch optimizer, e.g. adam, sgd, etc.\n",
        "    :param device: this can be \"cuda\" or \"cpu\"\n",
        "  \"\"\"\n",
        "  # set model to training mode\n",
        "  model.train()\n",
        "  # go through batches of data in data loader\n",
        "  for data in data_loader:\n",
        "    # fetch review and target from the dict\n",
        "    reviews = data[\"title\"]\n",
        "    up_votes = data[\"up_votes\"]\n",
        "    # move the data to device that we want to use\n",
        "    reviews = reviews.to(device, dtype=torch.long)\n",
        "    up_votes = up_votes.to(device, dtype=torch.float)\n",
        "    # clear the gradients\n",
        "    optimizer.zero_grad()\n",
        "    # make predictions from the model\n",
        "    predictions = model(reviews)\n",
        "    # calculate the loss\n",
        "    loss = nn.L1Loss()(\n",
        "      predictions,\n",
        "      up_votes.view(-1, 1)\n",
        "    )\n",
        "    # compute gradient of loss w.r.t.\n",
        "    # all parameters of the model that are trainable\n",
        "    loss.backward()\n",
        "    # single optimization step\n",
        "    optimizer.step()\n",
        "  scheduler.step()\n",
        "  \n",
        "def evaluate(data_loader, model, device):\n",
        "  # initialize empty lists to store predictions\n",
        "  # and targets\n",
        "  final_predictions = []\n",
        "  final_targets = []\n",
        "  # put the model in eval mode\n",
        "  model.eval()\n",
        "  # disable gradient calculation\n",
        "  with torch.no_grad():\n",
        "    for data in data_loader:\n",
        "      title = data[\"title\"]\n",
        "      up_votes = data[\"up_votes\"]\n",
        "      title = title.to(device, dtype=torch.long)\n",
        "      targets = targets.to(device, dtype=torch.float)\n",
        "      # make predictions\n",
        "      predictions = model(title)\n",
        "      # move predictions and targets to list\n",
        "      # we need to move predictions and targets to cpu too\n",
        "      predictions = predictions.cpu().numpy().tolist()\n",
        "      up_votes = data[\"up_votes\"].cpu().numpy().tolist()\n",
        "      final_predictions.extend(predictions)\n",
        "      final_targets.extend(targets)\n",
        "  \n",
        "  # return final predictions and targets\n",
        "  return final_predictions, final_targets"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcq5YQ_IEW7a"
      },
      "source": [
        "def load_vectors(fname):\n",
        "  # taken from: https://fasttext.cc/docs/en/english-vectors.html\n",
        "  fin = io.open(\n",
        "    fname,\n",
        "    'r',\n",
        "    encoding='utf-8',\n",
        "    newline='\\n',\n",
        "    errors='ignore'\n",
        "  )\n",
        "  n, d = map(int, fin.readline().split())\n",
        "  data = {}\n",
        "  for line in fin:\n",
        "    tokens = line.rstrip().split(' ')\n",
        "    data[tokens[0]] = list(map(float, tokens[1:]))\n",
        "  return data\n",
        "\n",
        "def create_embedding_matrix(word_index, embedding_dict):\n",
        "  \"\"\"\n",
        "    This function creates the embedding matrix.\n",
        "    :param word_index: a dictionary with word:index_value\n",
        "    :param embedding_dict: a dictionary with word:embedding_vector\n",
        "    :return: a numpy array with embedding vectors for all known words\n",
        "  \"\"\"\n",
        "  # initialize matrix with zeros\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "  # loop over all the words\n",
        "  for word, i in word_index.items():\n",
        "    # if word is found in pre-trained embeddings,\n",
        "    # update the matrix. if the word is not found,\n",
        "    # the vector is zeros!\n",
        "    if word in embedding_dict:\n",
        "      embedding_matrix[i] = embedding_dict[word]\n",
        "    \n",
        "  # return embedding matrix\n",
        "  return embedding_matrix\n",
        "\n",
        "def load_embeddings(word_index, embedding_file, vector_length=300):\n",
        "  \"\"\"\n",
        "    A general function to create embedding matrix\n",
        "    :param word_index: word:index dictionary\n",
        "    :param embedding_file: path to embeddings file\n",
        "    :param vector_length: length of vector\n",
        "  \"\"\"\n",
        "  max_features = len(word_index) + 1\n",
        "  words_to_find = list(word_index.keys())\n",
        "  more_words_to_find = []\n",
        "  \n",
        "  for wtf in words_to_find:\n",
        "    more_words_to_find.append(wtf)\n",
        "    more_words_to_find.append(str(wtf).capitalize())\n",
        "  more_words_to_find = set(more_words_to_find)\n",
        "  \n",
        "  def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "  \n",
        "  embeddings_index = dict(\n",
        "    get_coefs(*o.strip().split(\" \"))\n",
        "    for o in open(embedding_file)\n",
        "    if o.split(\" \")[0]\n",
        "    in more_words_to_find\n",
        "    and len(o) > 100\n",
        "  )\n",
        "  embedding_matrix = np.zeros((max_features, vector_length))\n",
        "  for word, i in word_index.items():\n",
        "    if i >= max_features:\n",
        "      continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is None:\n",
        "      embedding_vector = embeddings_index.get(\n",
        "        str(word).capitalize()\n",
        "      )\n",
        "    if embedding_vector is None:\n",
        "      embedding_vector = embeddings_index.get(\n",
        "        str(word).upper()\n",
        "      )\n",
        "    if (embedding_vector is not None\n",
        "      and len(embedding_vector) == vector_length):\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "  return embedding_matrix"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEv9cB7LE383"
      },
      "source": [
        "def run(train_df, valid_df, fold):\n",
        "  \"\"\"\n",
        "    Run training and validation for a given fold\n",
        "    and dataset\n",
        "    :param df: pandas dataframe with kfold column\n",
        "    :param fold: current fold, int\n",
        "  \"\"\"\n",
        "  \n",
        "  print(\"Fitting tokenizer\")\n",
        "  # we use tf.keras for tokenization\n",
        "  # you can use your own tokenizer and then you can\n",
        "  # get rid of tensorflow\n",
        "  tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "  tokenizer.fit_on_texts(df.title.values.tolist())\n",
        "  \n",
        "  # convert training data to sequences\n",
        "  # for example : \"bad movie\" gets converted to\n",
        "  # [24, 27] where 24 is the index for bad and 27 is the\n",
        "  # index for movie\n",
        "  xtrain = tokenizer.texts_to_sequences(train_df.title.values)\n",
        "  \n",
        "  # similarly convert validation data to\n",
        "  # sequences\n",
        "  xtest = tokenizer.texts_to_sequences(valid_df.title.values)\n",
        "  \n",
        "  # zero pad the training sequences given the maximum length\n",
        "  # this padding is done on left hand side\n",
        "  # if sequence is > MAX_LEN, it is truncated on left hand side too\n",
        "  xtrain = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    xtrain, maxlen=MAX_LEN\n",
        "  )\n",
        "  \n",
        "  # zero pad the validation sequences\n",
        "  xtest = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    xtest, maxlen=MAX_LEN\n",
        "  )\n",
        "  \n",
        "  # initialize dataset class for training\n",
        "  train_dataset = Dataset(\n",
        "    title=xtrain,\n",
        "    up_votes=train_df.up_votes.values\n",
        "  )\n",
        "  # create torch dataloader for training\n",
        "  # torch dataloader loads the data using dataset\n",
        "  # class in batches specified by batch size\n",
        "  train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE\n",
        "  )\n",
        "  # initialize dataset class for validation\n",
        "  valid_dataset = Dataset(\n",
        "    title=xtest,\n",
        "    up_votes=valid_df.up_votes.values\n",
        "  )\n",
        "\n",
        "  # create torch dataloader for validation\n",
        "  valid_data_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=VALID_BATCH_SIZE\n",
        "  )\n",
        "  # Skipping below because it is eating all RAM even with batch size = 1\n",
        "  # print(\"Loading embeddings\")\n",
        "  # # load embeddings as shown previously\n",
        "  # embedding_dict = load_vectors(\"/content/crawl-300d-2M.vec\")\n",
        "  # embedding_matrix = create_embedding_matrix(\n",
        "  #   tokenizer.word_index, embedding_dict\n",
        "  # )\n",
        "  # create torch device, since we use gpu, we are using cuda\n",
        "  device = torch.device(\"cuda\")\n",
        "  # fetch our LSTM model\n",
        "  model = Emb_CNN_LSTM_Model()\n",
        "  # send model to device\n",
        "  model.to(device)\n",
        "\n",
        "  # initialize Adam optimizer\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "  scheduler = get_scheduler(optimizer, 'CosineAnnealingWarmRestarts')\n",
        "  print(\"Training Model\")\n",
        "  # set best accuracy to zero\n",
        "  best_mae = 0\n",
        "  # set early stopping counter to zero\n",
        "  early_stopping_counter = 0\n",
        "  # train and validate for all epochs\n",
        "  for epoch in range(EPOCHS):\n",
        "    # train one epoch\n",
        "    train(train_data_loader, model, optimizer, device, scheduler)\n",
        "    # validate\n",
        "    outputs, targets = evaluate(\n",
        "      valid_data_loader, model, device\n",
        "    )\n",
        "    # use threshold of 0.5\n",
        "    # please note we are using linear layer and no sigmoid\n",
        "    # you should do this 0.5 threshold after sigmoid\n",
        "    # calculate accuracy\n",
        "    mae = metrics.mean_squared_error(targets, outputs)\n",
        "    print(\n",
        "      f\"FOLD:{fold}, Epoch: {epoch}, mae = {mae}\"\n",
        "    )\n",
        "    # simple early stopping\n",
        "    if mae < best_mae:\n",
        "      best_mae = mae\n",
        "    else:\n",
        "      early_stopping_counter += 1\n",
        "      \n",
        "    if early_stopping_counter > 2:\n",
        "      break"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsbtDBaPJkag"
      },
      "source": [
        "df['title'] = df['title'].astype(str)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvshR_5WHZvu"
      },
      "source": [
        "fold = 0\n",
        "for tr_in, val_in in KFold().split(df[['title', 'up_votes']]):\n",
        "  tr = df.loc[tr_in, ['title', 'up_votes']].reset_index(drop=True)\n",
        "  val = df.loc[val_in, ['title', 'up_votes']].reset_index(drop=True)\n",
        "  run(tr, val, fold)\n",
        "  fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovwa3cAkbs-U"
      },
      "source": [
        "# Note\n",
        "\n",
        "I have not trained it because I don't have GPU, the dataset is huge and the model I have created is big too but once I have access to a good machine, I can run it.\n",
        "\n",
        "Tried on Google colab, due to limitations getting out of memory error even with batch size = 1.\n",
        "Other than that everything is fine."
      ]
    }
  ]
}