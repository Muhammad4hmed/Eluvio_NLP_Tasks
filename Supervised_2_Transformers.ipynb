{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised_2_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhzJbzXRmNjP",
        "outputId": "c2a34a51-e4b6-4d56-b792-4872069549f4"
      },
      "source": [
        "!pip install transformers==3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3) (20.9)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.0.44)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3) (0.8.0rc4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PwU7fGbl1Tz",
        "outputId": "4dd9d4af-80af-4b3c-d66e-539e245f0219"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCMhRgTpmRwu"
      },
      "source": [
        "# Supervised Learning 2 - In this notebook, we are going to predict number of upvotes using state of the art transformers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3-T1yQLmKZa"
      },
      "source": [
        "import transformers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeTkTdSqmGgv"
      },
      "source": [
        "# this is the maximum number of tokens in the sentence\n",
        "MAX_LEN = 12\n",
        "# batch sizes is small because model is huge!\n",
        "TRAIN_BATCH_SIZE = 80\n",
        "VALID_BATCH_SIZE = 80\n",
        "# let's train for a maximum of 10 epochs\n",
        "EPOCHS = 10\n",
        "# define path to BERT model files\n",
        "BERT_PATH = \"bert-base-uncased\"\n",
        "# this is where you want to save the model\n",
        "MODEL_PATH = \"model.bin\"\n",
        "# training file\n",
        "TRAINING_FILE = \"/content/drive/MyDrive/Clean_Dataset.csv\"\n",
        "# define the tokenizer\n",
        "# we use tokenizer and model\n",
        "# from huggingface's transformers\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
        "  BERT_PATH,\n",
        "  do_lower_case=True\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7gIaFr3m7Gr"
      },
      "source": [
        "class BERTDataset:\n",
        "  def __init__(self, review, target):\n",
        "    \"\"\"\n",
        "    :param review: list or numpy array of strings\n",
        "    :param targets: list or numpy array which is binary\n",
        "    \"\"\"\n",
        "    self.review = review\n",
        "    self.target = target\n",
        "    # we fetch max len and tokenizer from config.py\n",
        "    self.tokenizer = TOKENIZER\n",
        "    self.max_len = MAX_LEN\n",
        "  \n",
        "  def __len__(self):\n",
        "    # this returns the length of dataset\n",
        "    return len(self.review)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    # for a given item index, return a dictionary\n",
        "    # of inputs\n",
        "    review = str(self.review[item])\n",
        "    review = \" \".join(review.split())\n",
        "    # encode_plus comes from hugginface's transformers\n",
        "    # and exists for all tokenizers they offer\n",
        "    # it can be used to convert a given string\n",
        "    # to ids, mask and token type ids which are\n",
        "    # needed for models like BERT\n",
        "    # here, review is a string\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      None,\n",
        "      truncation = True,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      pad_to_max_length=True,\n",
        "    )\n",
        "    # ids are ids of tokens generated\n",
        "    # after tokenizing reviews\n",
        "    ids = inputs[\"input_ids\"]\n",
        "    # mask is 1 where we have input\n",
        "    # and 0 where we have padding\n",
        "    mask = inputs[\"attention_mask\"]\n",
        "    # token type ids behave the same way as\n",
        "    # mask in this specific case\n",
        "    # in case of two sentences, this is 0\n",
        "    # for first sentence and 1 for second sentence\n",
        "    token_type_ids = inputs[\"token_type_ids\"]\n",
        "    # now we return everything\n",
        "    # note that ids, mask and token_type_ids\n",
        "    # are all long datatypes and targets is float\n",
        "    return {\n",
        "      \"ids\": torch.tensor(\n",
        "          ids, dtype=torch.long\n",
        "        ),\n",
        "      \"mask\": torch.tensor(\n",
        "          mask, dtype=torch.long\n",
        "      ),\n",
        "      \"token_type_ids\": torch.tensor(\n",
        "          token_type_ids, dtype=torch.long\n",
        "      ),\n",
        "      \"targets\": torch.tensor(\n",
        "          self.target[item], dtype=torch.float\n",
        "      )\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLVozHYunFd0"
      },
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BERTBaseUncased, self).__init__()\n",
        "    # we fetch the model from the BERT_PATH defined in\n",
        "    # config.py\n",
        "    self.bert = transformers.BertModel.from_pretrained(\n",
        "      BERT_PATH\n",
        "    )\n",
        "    # add a dropout for regularization\n",
        "    self.bert_drop = nn.Dropout(0.3)\n",
        "    # a simple linear layer for output\n",
        "    # yes, there is only one output\n",
        "    self.out = nn.Linear(768, 1)\n",
        "    \n",
        "  def forward(self, ids, mask, token_type_ids):\n",
        "    # BERT in its default settings returns two outputs\n",
        "    # last hidden state and output of bert pooler layer\n",
        "    # we use the output of the pooler which is of the size\n",
        "    # (batch_size, hidden_size)\n",
        "    # hidden size can be 768 or 1024 depending on\n",
        "    # if we are using bert base or large respectively\n",
        "    # in our case, it is 768\n",
        "    # note that this model is pretty simple\n",
        "    # you might want to use last hidden state\n",
        "    # or several hidden states\n",
        "    _, o2 = self.bert(\n",
        "      ids,\n",
        "      attention_mask=mask,\n",
        "      token_type_ids=token_type_ids\n",
        "    )\n",
        "    # pass through dropout layer\n",
        "    bo = self.bert_drop(o2)\n",
        "    # pass through linear layer\n",
        "    output = self.out(bo)\n",
        "    # return output\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKB1LKdcnWFa"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "  \"\"\"\n",
        "    This function returns the loss.\n",
        "    :param outputs: output from the model (real numbers)\n",
        "    :param targets: input targets (binary)\n",
        "  \"\"\"\n",
        "  return nn.L1Loss()(outputs, targets.view(-1, 1))\n",
        "\n",
        "def get_scheduler(optimizer, scheduler):\n",
        "  if scheduler=='ReduceLROnPlateau':\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=4, verbose=True, eps=1e-6)\n",
        "  elif scheduler=='CosineAnnealingLR':\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6, last_epoch=-1)\n",
        "  elif scheduler=='CosineAnnealingWarmRestarts':\n",
        "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1)\n",
        "  return scheduler\n",
        "\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "  \"\"\"\n",
        "    This is the training function which trains for one epoch\n",
        "    :param data_loader: it is the torch dataloader object\n",
        "    :param model: torch model, bert in our case\n",
        "    :param optimizer: adam, sgd, etc\n",
        "    :param device: can be cpu or cuda\n",
        "    :param scheduler: learning rate scheduler\n",
        "  \"\"\"\n",
        "  # put the model in training mode\n",
        "  model.train()\n",
        "  # loop over all batches\n",
        "  for d in tqdm.tqdm(data_loader):\n",
        "    # extract ids, token type ids and mask\n",
        "    # from current batch\n",
        "    # also extract targets\n",
        "    ids = d[\"ids\"]\n",
        "    token_type_ids = d[\"token_type_ids\"]\n",
        "    mask = d[\"mask\"]\n",
        "    targets = d[\"targets\"]\n",
        "    # move everything to specified device\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    targets = targets.to(device, dtype=torch.float)\n",
        "    # zero-grad the optimizer\n",
        "    optimizer.zero_grad()\n",
        "    # pass through the model\n",
        "    outputs = model(\n",
        "      ids=ids,\n",
        "      mask=mask,\n",
        "      token_type_ids=token_type_ids\n",
        "    )\n",
        "    # calculate loss\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    # backward step the loss\n",
        "    loss.backward()\n",
        "    # step optimizer\n",
        "    optimizer.step()\n",
        "  # step scheduler\n",
        "  scheduler.step()\n",
        "  \n",
        "def eval_fn(data_loader, model, device):\n",
        "  \"\"\"\n",
        "    this is the validation function that generates\n",
        "    predictions on validation data\n",
        "    :param data_loader: it is the torch dataloader object\n",
        "    :param model: torch model, bert in our case\n",
        "    :param device: can be cpu or cuda\n",
        "    :return: output and targets\n",
        "  \"\"\"\n",
        "  # put model in eval mode\n",
        "  model.eval()\n",
        "  # initialize empty lists for\n",
        "  # targets and outputs\n",
        "  fin_targets = []\n",
        "  fin_outputs = []\n",
        "  # use the no_grad scope\n",
        "  # its very important else you might\n",
        "  # run out of gpu memory\n",
        "  with torch.no_grad():\n",
        "    # this part is same as training function\n",
        "    # except for the fact that there is no\n",
        "    # zero_grad of optimizer and there is no loss\n",
        "    # calculation or scheduler steps.\n",
        "    for d in tqdm.tqdm(data_loader):\n",
        "      ids = d[\"ids\"]\n",
        "      token_type_ids = d[\"token_type_ids\"]\n",
        "      mask = d[\"mask\"]\n",
        "      targets = d[\"targets\"]\n",
        "      ids = ids.to(device, dtype=torch.long)\n",
        "      token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "      mask = mask.to(device, dtype=torch.long)\n",
        "      targets = targets.to(device, dtype=torch.float)\n",
        "      outputs = model(\n",
        "        ids=ids,\n",
        "        mask=mask,\n",
        "        token_type_ids=token_type_ids\n",
        "      )\n",
        "      # convert targets to cpu and extend the final list\n",
        "      targets = targets.cpu().detach()\n",
        "      fin_targets.extend(targets.numpy().tolist())\n",
        "      # convert outputs to cpu and extend the final list\n",
        "      outputs = torch.sigmoid(outputs).cpu().detach()\n",
        "      fin_outputs.extend(outputs.numpy().tolist())\n",
        "  return fin_outputs, fin_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56KViT9gnd7o"
      },
      "source": [
        "def train():\n",
        "  # this function trains the model\n",
        "\n",
        "  # read the training file and fill NaN values with \"none\"\n",
        "  # you can also choose to drop NaN values in this\n",
        "  # specific dataset\n",
        "  dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
        "  # we split the data into single training\n",
        "  # and validation fold\n",
        "  df_train, df_valid = model_selection.train_test_split(\n",
        "    dfx,\n",
        "    test_size=0.1,\n",
        "    random_state=42\n",
        "  )\n",
        "  # reset index\n",
        "  df_train = df_train.reset_index(drop=True)\n",
        "  df_valid = df_valid.reset_index(drop=True)\n",
        "  # initialize BERTDataset from dataset.py\n",
        "  # for training dataset\n",
        "  train_dataset = BERTDataset(\n",
        "    review=df_train.title.values,\n",
        "    target=df_train.up_votes.values\n",
        "  )\n",
        "  # create training dataloader\n",
        "  train_data_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    num_workers=2\n",
        "  )\n",
        "  # initialize BERTDataset from dataset.py\n",
        "  # for validation dataset\n",
        "  valid_dataset = BERTDataset(\n",
        "    review=df_valid.title.values,\n",
        "    target=df_valid.up_votes.values\n",
        "  )\n",
        "  # create validation data loader\n",
        "  valid_data_loader = torch.utils.data.DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    num_workers=1\n",
        "  )\n",
        "  # initialize the cuda device\n",
        "  # use cpu if you dont have GPU\n",
        "  device = torch.device(\"cuda\")\n",
        "  # load model and send it to the device\n",
        "  model = BERTBaseUncased()\n",
        "  model.to(device)\n",
        "  # create parameters we want to optimize\n",
        "  # we generally dont use any decay for bias\n",
        "  # and weight layers\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "  \n",
        "  optimizer_parameters = [\n",
        "    {\n",
        "      \"params\": [\n",
        "        p for n, p in param_optimizer if\n",
        "        not any(nd in n for nd in no_decay)\n",
        "      ],\n",
        "      \"weight_decay\": 0.001,\n",
        "    },\n",
        "    {\n",
        "      \"params\": [\n",
        "        p for n, p in param_optimizer if\n",
        "        any(nd in n for nd in no_decay)\n",
        "      ],\n",
        "      \"weight_decay\": 0.0,\n",
        "    },\n",
        "  ]\n",
        "  # calculate the number of training steps\n",
        "  # this is used by scheduler\n",
        "  num_train_steps = int(\n",
        "    len(df_train) / TRAIN_BATCH_SIZE * EPOCHS\n",
        "  )\n",
        "  # AdamW optimizer\n",
        "  # AdamW is the most widely used optimizer\n",
        "  # for transformer based networks\n",
        "  optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
        "  # fetch a scheduler\n",
        "  # you can also try using reduce lr on plateau\n",
        "  scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_train_steps\n",
        "  )\n",
        "  # if you have multiple GPUs\n",
        "  # model model to DataParallel\n",
        "  # to use multiple GPUs\n",
        "  model = nn.DataParallel(model)\n",
        "  # start training the epochs\n",
        "  best_mse = 100000\n",
        "  for epoch in range(EPOCHS):\n",
        "    train_fn(\n",
        "      train_data_loader, model, optimizer, device, scheduler\n",
        "    )\n",
        "    outputs, targets = eval_fn(\n",
        "      valid_data_loader, model, device\n",
        "    )\n",
        "    mse = metrics.mean_squared_error(targets, outputs)\n",
        "    print(f\"mse = {mse}\")\n",
        "    if mse < best_mse:\n",
        "      torch.save(model.state_dict(), '/content/model.pth')\n",
        "      best_mse = mse\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j71s0HjtobqR",
        "outputId": "9e912fb8-002b-461c-e69b-5d68d95ecddc"
      },
      "source": [
        "train()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:40<00:00,  5.11it/s]\n",
            "100%|██████████| 637/637 [00:38<00:00, 16.57it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312367.68590610044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:40<00:00,  5.11it/s]\n",
            "100%|██████████| 637/637 [00:38<00:00, 16.49it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312369.0896893257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:41<00:00,  5.11it/s]\n",
            "100%|██████████| 637/637 [00:38<00:00, 16.62it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312368.4722742154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:41<00:00,  5.11it/s]\n",
            "100%|██████████| 637/637 [00:38<00:00, 16.54it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312369.89573852735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:41<00:00,  5.11it/s]\n",
            "100%|██████████| 637/637 [00:38<00:00, 16.70it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312370.6128814866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:39<00:00,  5.12it/s]\n",
            "100%|██████████| 637/637 [00:37<00:00, 16.90it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312367.6099578829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:39<00:00,  5.12it/s]\n",
            "100%|██████████| 637/637 [00:37<00:00, 16.97it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312366.81488932535\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:39<00:00,  5.12it/s]\n",
            "100%|██████████| 637/637 [00:38<00:00, 16.74it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312366.5749625498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:38<00:00,  5.12it/s]\n",
            "100%|██████████| 637/637 [00:37<00:00, 17.13it/s]\n",
            "  0%|          | 0/5729 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312367.0092939762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5729/5729 [18:37<00:00,  5.13it/s]\n",
            "100%|██████████| 637/637 [00:37<00:00, 17.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mse = 312366.9248057609\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}